{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search - A Collection of Similarity Search Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity search in machine learning refers to the task of finding objects that are similar to a given query object within a dataset. In this notebook, I'll be implementing multiple Similarity Search algorithms as describe in <a href=\"https://www.youtube.com/watch?v=AY62z7HrghY&list=PLIUOU7oqGTLhlWpTz4NnuT3FekouIVlqc&index=1\" target=\"__blank__\">this</a> YouTube playlist by James Briggs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard similarity is a measure used to compare the similarity and diversity of sample sets. It is defined as the size of the intersection of the sets divided by the size of the union of the sets.\n",
    "\n",
    "Mathematically, the Jaccard similarity J(A,B) between two sets A and B is calculated as:\n",
    "\n",
    "$$\n",
    "J(A, B) = | {A \\cap B} | \\div | {A \\cup B} |\n",
    "$$\n",
    "\n",
    "The Jaccard similarity ranges from 0 to 1. A value of 1 indicates that the sets are identical, while a value of 0 indicates that the sets have no elements in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 1.0\n"
     ]
    }
   ],
   "source": [
    "set1 = {1, 2, 3}\n",
    "set2 = {1, 2, 3}\n",
    "\n",
    "# 3 / 3\n",
    "similarity = jaccard_similarity(set1, set2)\n",
    "print(\"Jaccard Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "set1 = {1, 2, 3, 4, 5}\n",
    "set2 = {3, 4, 5, 6, 7}\n",
    "\n",
    "# 3 / 7\n",
    "similarity = jaccard_similarity(set1, set2)\n",
    "print(\"Jaccard Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 0.5\n"
     ]
    }
   ],
   "source": [
    "set1 = {\"A\", \"B\", \"C\"}\n",
    "set2 = {\"A\", \"B\", \"D\"}\n",
    "\n",
    "# 2 / 4 \n",
    "similarity = jaccard_similarity(set1, set2)\n",
    "print(\"Jaccard Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W-shingling\n",
    "\n",
    "W-shingling is a technique used in text analysis and information retrieval to represent documents as sets of shingles. Shingles, also known as n-grams, are contiguous sequences of words or characters in a document.\n",
    "\n",
    "In w-shingling, a document is divided into w consecutive words (or characters) at a time, forming overlapping or non-overlapping shingles. The choice of w determines the size of the shingles.\n",
    "\n",
    "For example, suppose we have the following sentence:\n",
    "\n",
    "\"Machine learning is a subset of artificial intelligence.\"\n",
    "\n",
    "If we use 3-shingling (trigrams) with overlapping shingles, the shingles would be:\n",
    "\n",
    "- \"Machine learning is\"\n",
    "- \"learning is a\"\n",
    "- \"is a subset\"\n",
    "- \"a subset of\"\n",
    "- \"subset of artificial\"\n",
    "- \"of artificial intelligence\"\n",
    "\n",
    "If we use non-overlapping shingles, the shingles would be:\n",
    "\n",
    "- \"Machine learning is\"\n",
    "- \"a subset of\"\n",
    "- \"artificial intelligence\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how the shingles can be generated,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shingles:\n",
      "learning is a\n",
      "Machine learning is\n",
      "subset of artificial\n",
      "is a subset\n",
      "of artificial intelligence.\n",
      "a subset of\n"
     ]
    }
   ],
   "source": [
    "def generate_shingles(text, w, overlap=False):\n",
    "    shingles = set()\n",
    "    words = text.split()\n",
    "    \n",
    "    if overlap:\n",
    "        for i in range(len(words) - w + 1):\n",
    "            shingle = \" \".join(words[i:i+w])\n",
    "            shingles.add(shingle)\n",
    "    else:\n",
    "        for i in range(0, len(words), w):\n",
    "            shingle = \" \".join(words[i:i+w])\n",
    "            shingles.add(shingle)\n",
    "    \n",
    "    return shingles\n",
    "\n",
    "text = \"Machine learning is a subset of artificial intelligence.\"\n",
    "w = 3  \n",
    "overlap = True\n",
    "\n",
    "shingles = generate_shingles(text, w, overlap)\n",
    "\n",
    "print(\"Shingles:\")\n",
    "for shingle in shingles:\n",
    "    print(shingle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how the generated shingles can be used with Jaccard similarity for similarity search,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shingles Text1: {'learning is a', 'Machine learning is', 'subset of artificial', 'is a subset', 'of artificial intelligence', 'a subset of'}\n",
      "Shingles Text2: {'is a subset', 'intelligence is a', 'subset of computer', 'of computer science', 'Artificial intelligence is', 'a subset of'}\n",
      "Jaccard Similarity: 0.2\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Machine learning is a subset of artificial intelligence\"\n",
    "text2 = \"Artificial intelligence is a subset of computer science\"\n",
    "\n",
    "# Generate shingles for both texts\n",
    "shingles_text1 = generate_shingles(text1, w, overlap)\n",
    "shingles_text2 = generate_shingles(text2, w, overlap)\n",
    "\n",
    "print(f\"Shingles Text1: {shingles_text1}\")\n",
    "print(f\"Shingles Text2: {shingles_text2}\")\n",
    "\n",
    "# Calculate Jaccard similarity between the sets of shingles\n",
    "# Jaccard similarity = 2 / 10\n",
    "similarity = jaccard_similarity(shingles_text1, shingles_text2)\n",
    "\n",
    "# Print the Jaccard similarity\n",
    "print(\"Jaccard Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Levenshtein distance\n",
    "\n",
    "Levenshtein distance, also known as edit distance, is a measure of the similarity between two strings. It is defined as the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other.\n",
    "\n",
    "For example, the Levenshtein distance between the strings \"kitten\" and \"sitting\" is 3, as illustrated by the following sequence of edits:\n",
    "\n",
    "- Replace 'k' with 's' (substitution)\n",
    "- Insert 's' at the beginning\n",
    "- Replace 'e' with 'i'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance between 'kitten' and 'sitting': 3\n"
     ]
    }
   ],
   "source": [
    "def levenshtein_distance(s1, s2):\n",
    "    distance_matrix = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]\n",
    "\n",
    "    # Initialize the first row and column of the matrix\n",
    "    # max(i,j) if min(i, j) = 0\n",
    "    for i in range(len(s1) + 1):\n",
    "        distance_matrix[i][0] = i\n",
    "    for j in range(len(s2) + 1):\n",
    "        distance_matrix[0][j] = j\n",
    "\n",
    "    # Fill in the rest of the matrix\n",
    "    for i in range(1, len(s1) + 1):\n",
    "        for j in range(1, len(s2) + 1):\n",
    "            if s1[i - 1] == s2[j - 1]:\n",
    "                substitution_cost = 0\n",
    "            else:\n",
    "                substitution_cost = 1\n",
    "            distance_matrix[i][j] = min(distance_matrix[i - 1][j] + 1, # deletion\n",
    "                                        distance_matrix[i][j - 1] + 1, # insertion\n",
    "                                        distance_matrix[i - 1][j - 1] + substitution_cost) # substitution\n",
    "\n",
    "    # Return the bottom-right cell of the matrix\n",
    "    return distance_matrix[len(s1)][len(s2)]\n",
    "\n",
    "# Example usage:\n",
    "s1 = \"kitten\"\n",
    "s2 = \"sitting\"\n",
    "distance = levenshtein_distance(s1, s2)\n",
    "print(\"Levenshtein Distance between '{}' and '{}': {}\".format(s1, s2, distance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're looking for a fast implementation of Levenshtein Distance, check out this <a href=\"https://pypi.org/project/python-Levenshtein/\" target=\"__blank__\">Python module</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a method for weighting the importance of words in documents. It assigns higher weights to terms that are frequent in a document but rare across the entire corpus. This technique is commonly used in similarity search by representing documents as numerical vectors based on their TF-IDF scores. Cosine similarity is then applied to compare these vectors, determining the similarity between documents.\n",
    "\n",
    "The steps for calculating TF-IDF are as follows:\n",
    "\n",
    "1. Term Frequency (TF): Calculate the frequency of each term in a document relative to the total number of terms in that document.\n",
    "\n",
    "2. Inverse Document Frequency (IDF): Calculate the logarithm of the ratio of the total number of documents to the number of documents containing each term.\n",
    "\n",
    "3. TF-IDF Score: Multiply the TF and IDF values for each term in each document to get the TF-IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF scores: [{'hello': 0.6666666666666666, 'world': 0.3333333333333333}, {'hello': 0.5, 'python': 0.5}, {'python': 0.5, 'world': 0.5}]\n",
      "IDF scores: {'hello': 1.4054651081081644, 'world': 1.4054651081081644, 'python': 1.4054651081081644}\n",
      "Document 1 TF-IDF scores:\n",
      "hello: 0.9370\n",
      "world: 0.4685\n",
      "\n",
      "Document 2 TF-IDF scores:\n",
      "hello: 0.7027\n",
      "python: 0.7027\n",
      "\n",
      "Document 3 TF-IDF scores:\n",
      "python: 0.7027\n",
      "world: 0.7027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_tf(term_freq):\n",
    "    total_terms = sum(term_freq.values())\n",
    "    tf_scores = {term: freq / total_terms for term, freq in term_freq.items()}\n",
    "    return tf_scores\n",
    "\n",
    "def calculate_idf(documents):\n",
    "    total_docs = len(documents)\n",
    "    all_terms = set(term for doc in documents for term in doc)\n",
    "    idf_scores = {}\n",
    "\n",
    "    for term in all_terms:\n",
    "        doc_freq = sum(1 for doc in documents if term in doc)\n",
    "        idf_scores[term] = math.log(total_docs / doc_freq) + 1\n",
    "    return idf_scores\n",
    "\n",
    "def calculate_tf_idf(tf_scores, idf_scores):\n",
    "    tf_idf_scores = {}\n",
    "    for term, tf_score in tf_scores.items():\n",
    "        tf_idf_scores[term] = tf_score * idf_scores[term]\n",
    "    return tf_idf_scores\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    [\"hello\", \"world\", \"hello\"],\n",
    "    [\"hello\", \"python\"],\n",
    "    [\"python\", \"world\"]\n",
    "]\n",
    "\n",
    "# Calculate TF scores for each document\n",
    "tf_scores_list = [calculate_tf(Counter(doc)) for doc in documents]\n",
    "\n",
    "print(f\"TF scores: {tf_scores_list}\")\n",
    "\n",
    "# Calculate IDF scores for the entire corpus\n",
    "idf_scores = calculate_idf(documents)\n",
    "\n",
    "print(f\"IDF scores: {idf_scores}\")\n",
    "\n",
    "# Calculate TF-IDF scores for each document\n",
    "tf_idf_scores_list = [calculate_tf_idf(tf_scores, idf_scores) for tf_scores in tf_scores_list]\n",
    "\n",
    "# Print TF-IDF scores for each document\n",
    "for i, tf_idf_scores in enumerate(tf_idf_scores_list):\n",
    "    print(f\"Document {i+1} TF-IDF scores:\")\n",
    "    for term, score in tf_idf_scores.items():\n",
    "        print(f\"{term}: {score:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Matching 25 (BM25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BM25, or Best Matching 25, is a ranking function used to estimate the relevance of documents to a given search query. It is an improvement over the TF-IDF model and is widely used in information retrieval and search engines. BM25 takes into account factors such as term frequency, document length, and document frequency.\n",
    "\n",
    "The score of a document $D$ given a query $Q$ which contains $q_1, q_2, ..., q_n$ is given by:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\text{BM25}(D,Q) = \\sum_{i=1}^{n} \\text{IDF}(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}})} \\end{equation}\n",
    "$$\n",
    "\n",
    "where is $f(q_i, D)$'s term frequency in the document $D$, $|D|$ is the length of the document $D$ in words, and avgdl is the average document length in the text collection from which documents are drawn. $k_1$ and $b$ are free parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 0.9808292530117264, 1: 0.0, 2: 0.0})\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "class BM25:\n",
    "    def __init__(self, corpus, k1=1.5, b=0.75):\n",
    "        self.corpus = corpus\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.avgdl = sum(map(len, corpus)) / len(corpus)\n",
    "        self.idf = self.calculate_idf()\n",
    "        \n",
    "    def calculate_idf(self):\n",
    "        idf = {}\n",
    "        for document in self.corpus:\n",
    "            word_set = set(document)\n",
    "            for word in word_set:\n",
    "                idf[word] = idf.get(word, 0) + 1\n",
    "        for word, freq in idf.items():\n",
    "            idf[word] = math.log((len(self.corpus) - freq + 0.5) / (freq + 0.5) + 1)\n",
    "        return idf\n",
    "    \n",
    "    def get_score(self, query):\n",
    "        scores = Counter()\n",
    "        for word in query:\n",
    "            if word in self.idf:\n",
    "                for i, doc in enumerate(self.corpus):\n",
    "                    f = doc.count(word)\n",
    "                    dl = len(doc)\n",
    "                    scores[i] += self.idf[word] * (f * (self.k1 + 1)) / (f + self.k1 * (1 - self.b + self.b * dl / self.avgdl))\n",
    "        return scores\n",
    "\n",
    "\n",
    "corpus = [['hello', 'world'], ['world', 'python'], ['python', 'example']]\n",
    "bm25 = BM25(corpus)\n",
    "query = ['hello']\n",
    "scores = bm25.get_score(query)\n",
    "\n",
    "# Scores for documents/corpus\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAISS (Facebook AI Similarity Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAISS, or Facebook AI Similarity Search, is an efficient library for similarity search and clustering of dense vectors. It is particularly popular for handling large-scale vector databases. FAISS is built to perform similarity searches using methods like k-nearest neighbors (KNN) and approximate nearest neighbors (ANN) efficiently. Here are the basic steps to use FAISS:\n",
    "\n",
    "- Prepare data: Prepare your dataset in a suitable format (usually a numpy array).\n",
    "- Choose index type: Select an appropriate index type based on your needs (e.g., Flat, IVF, LSH).\n",
    "- Build index: Build the index using the chosen index type and the prepared data.\n",
    "- Query: Perform queries using the index to find similar vectors or data points.\n",
    "- Evaluate results: Evaluate and analyze the results obtained from the queries.\n",
    "- Fine-tune parameters: Optionally fine-tune parameters to optimize performance for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install faiss\n",
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "d = 500\n",
    "n_input = 100000\n",
    "data = np.random.rand(n_input, d).astype(np.float32)\n",
    "\n",
    "index = faiss.IndexFlatL2(d)\n",
    "print(index.ntotal) # 0 because we have not added any data in yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "# Add data vectors to the index\n",
    "index.add(data)\n",
    "\n",
    "print(index.ntotal) # Same as n_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's query our index,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors:\n",
      "Index: 65415 Distance: 66.50713\n",
      "Index: 41447 Distance: 67.38054\n",
      "Index: 46569 Distance: 68.09364\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 28 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define a query vector\n",
    "query = np.random.rand(1, d).astype(np.float32)\n",
    "\n",
    "# Perform a nearest neighbor search\n",
    "k = 3  # Number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query, k)\n",
    "\n",
    "print(\"Nearest neighbors:\")\n",
    "for i in range(k):\n",
    "    print(\"Index:\", indices[0][i], \"Distance:\", distances[0][i])\n",
    "    # print(\"Vector:\", data[indices[0][i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the computation time to query is large given our index is large. We can improve speed of our query using a quantizer,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is trained? True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "nlist = 1000\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "index = faiss.IndexIVFFlat(quantizer, d, nlist)\n",
    "\n",
    "# Train using the embeddings\n",
    "index.train(data)\n",
    "\n",
    "print(f\"Is trained? {index.is_trained}\")\n",
    "print(f\"{index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "# Adding to the index\n",
    "index.add(data)\n",
    "print(f\"{index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors:\n",
      "Index: 41447 Distance: 67.38054\n",
      "Index: 23145 Distance: 70.838776\n",
      "Index: 20121 Distance: 70.900505\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k = 3 \n",
    "distances, indices = index.search(query, k)\n",
    "\n",
    "print(\"Nearest neighbors:\")\n",
    "for i in range(k):\n",
    "    print(\"Index:\", indices[0][i], \"Distance:\", distances[0][i])\n",
    "    # print(\"Vector:\", data[indices[0][i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation time has gone down by a lot but we do suffer in accuracy by quantizing our data by a lot (splitting into 1000 buckets)! It is important to choose a value for `n_list` that is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an argument called `nprobe` to be more exhaustive during our search by looking into nearby buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors:\n",
      "Index: 65415 Distance: 66.50713\n",
      "Index: 41447 Distance: 67.38054\n",
      "Index: 46569 Distance: 68.09364\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "index.nprobe = 400\n",
    "\n",
    "k = 3 \n",
    "distances, indices = index.search(query, k)\n",
    "\n",
    "print(\"Nearest neighbors:\")\n",
    "for i in range(k):\n",
    "    print(\"Index:\", indices[0][i], \"Distance:\", distances[0][i])\n",
    "    # print(\"Vector:\", data[indices[0][i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit more accurate now although our computation time has gone up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, FAISS also provides a way to quantize the data using product quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of centroids in the constructed vector ( d % m should be 0 )\n",
    "m = 5\n",
    "\n",
    "# Number of bits insde centroid\n",
    "bits = 8\n",
    "\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "index = faiss.IndexIVFPQ(quantizer, d, nlist, m, bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the embedding data\n",
    "index.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors:\n",
      "Index: 45632 Distance: 42.138412\n",
      "Index: 14152 Distance: 42.256477\n",
      "Index: 84948 Distance: 42.453526\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 998 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "distances, indices = index.search(query, k)\n",
    "\n",
    "print(\"Nearest neighbors:\")\n",
    "for i in range(k):\n",
    "    print(\"Index:\", indices[0][i], \"Distance:\", distances[0][i])\n",
    "    # print(\"Vector:\", data[indices[0][i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its a lot faster but the accuracy has suffered compared to a fully exhaustive search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's move onto other kind of indexes that are available in FAISS,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- IndexFlatL2 - A type of FAISS index that creates a flat index structure using L2 (Euclidean distance) metric for similarity search. It directly stores all vectors in a single structure without any hierarchical organization, making it simple but potentially less efficient for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 141 ms\n",
      "Wall time: 73.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(data)\n",
    "\n",
    "k=10\n",
    "distances, indices = index.search(query, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a baseline for future comparisons,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[65415, 41447, 46569, 99294, 46063, 76476, 73468, 11862, 24210, 85898]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = indices[0].tolist()\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- IndexFlatIP - FAISS index optimized for inner product similarity search, employing a flat structure to efficiently compute inner products between query vectors and dataset vectors, commonly used in recommendation systems and information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "distances, indices = index.search(query, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.in1d(baseline, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Locality Sensitive Hashing (LSH) - LSH is a method for approximate nearest neighbor search. It hashes data points into hash buckets in a way that similar points are likely to hash to the same bucket. This enables efficient retrieval of approximate nearest neighbors by searching within these hash buckets rather than comparing all data points directly. LSH is especially useful for high-dimensional data where traditional exact nearest neighbor search methods become computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbits = d * 2\n",
    "index = faiss.IndexLSH(d, nbits)\n",
    "index.add(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 4.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "distances, indices = index.search(query, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.in1d(baseline, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hierarchical Navigable Small World (HNSW) - A type of FAISS index that constructs a hierarchical graph structure to organize data points. It uses navigable small-world graphs to efficiently perform approximate nearest neighbor searches, especially effective for high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 16 # number of connections each vertex as\n",
    "ef_search = 8 # depth of our search\n",
    "ef_construction = 64 # how efficiently and accurately are we going to build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexHNSWFlat(d, M)\n",
    "index.hnsw.efSearch = ef_search\n",
    "index.hnsw.efConstruction = ef_construction\n",
    "\n",
    "index.add(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "distances, indices = index.search(query, k)\n",
    "np.in1d(baseline, indices)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
